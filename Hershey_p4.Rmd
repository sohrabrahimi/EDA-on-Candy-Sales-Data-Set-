---
title: "P4_Hershey"
author: "Sohrab Rahimi"
date: "January 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation

In this project I will explore a candy sale data set. This data includes infomation about the candy sale for 1253  markets in four major cities (i.e. Philadelphia, Boston, DC and Pittsburgh) for 31 types of chocolate. The geographic information about the markets are also included. 

I will try to answer the following questions in this project: 

1- Which Brands sell more and in which cities?
2- Which zipcodes consume more candy on average? 
3- which markets sell more candies and what kinds of candies?
4- Which candies are more likely to be sold together? 
5- How different markets compare together interms of their candy sales?
6- What parts of the city consume more chocolate? how different neighborhoods compare in terms of their chocolate consumption? 



First we load all the required packages into R: 
```{r Hershey, results='hide', echo= FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(dplyr)
library(tidyr)
library(psych)
library(arules)
library(arulesViz)
library(gridExtra)
library(reshape2)
library(Hmisc)
library(corrplot)
library(data.table)
library(lattice)
library(ggmap)
library(SNFtool)
library(stringr)
library(cluster)
library(ggdendro)
library(maptools)
library(mapdata)
library(knitr)
data = read.csv("C:/Users/sur216/Box Sync/school stuff/Udacity (sur216@psu.edu)/Data Analyst/p4_hershey/CANDIES_STORES.csv",header = T)
data = data[,-1]
```


The data includes 1253 rows each representing one store. For each store the sales data for 31 brands of chocolate has been provided. 
```{r data, echo=FALSE}
names(data)
dim(data)
# make a table of variable classes
tab = as.data.frame(lapply(data, class))
kable(tab[,1:5], format = "markdown")
```

the rest of 32 variables are 32 candy brands sales in USD (integer). The data is collected for four cities: 

```{r data_city, echo=FALSE}
# city levels 
unique(data$CITY)
```
The data includes the candy sale data for 37 chain-markets: 

```{r data_stores, echo=FALSE}
# Store levels
unique(data$STORE)
```

for 30 candy brands: 

```{r data_brands, echo=FALSE}
# Candy brand levels
names(data)[6:36]
```
## which markets sell more candies and what kinds of candies?

In order to get a sense of the candy sales in each city I will first create a "TOTAL" column where I add the sales for all types of candy for each market. Based on the plot below we can see that the target store sells highest in all except for Philadelphia. For Philadelphia alone, BJs wholesale club is slightly more than target on average. 
```{r city, warning=FALSE}

data$TOTAL = rowSums(subset(data,
                            select = -c(STORE,LONG,LAT,ZIP.CODE,CITY)))

candy_by_city = data %>% 
  group_by(CITY) %>% 
  summarise (sales_mean = mean(TOTAL),
             sales_median = median(TOTAL), 
             sales_sum = sum(TOTAL), n = n()) 
candy_by_city = as.data.frame(candy_by_city)
candy_by_city 
# convert to long format
candy_by_city_long = gather(candy_by_city, sales_mean,sales_median,sales_sum,
                            sales_mean:sales_median:sales_sum, factor_key=TRUE)
names(candy_by_city_long)[names(candy_by_city_long) == 'sales_mean'] <- "Measure"
names(candy_by_city_long)[names(candy_by_city_long) == 'sales_median'] <- "value"


```
We can do the same thing for stores to compare different types of stores in terms of their success in selling candies. 

```{r store brand,echo = FALSE, warning=FALSE}


# we will do the same thing for stores

candy_by_store = data %>% 
  group_by(STORE) %>% 
  summarise (sales_mean = mean(TOTAL),
             sales_median = median(TOTAL), 
             sales_sum = sum(TOTAL), n = n()) 
candy_by_store = as.data.frame(candy_by_store)


# convert to long format
candy_by_store_long = gather(candy_by_store, sales_mean,sales_median,sales_sum,
                            sales_mean:sales_median:sales_sum, factor_key=TRUE)
names(candy_by_store_long)[names(candy_by_store_long) == 'sales_mean'] <- "Measure"
names(candy_by_store_long)[names(candy_by_store_long) == 'sales_median'] <- "value"

```

```{r brands,echo = FALSE, warning = FALSE, fig.width=12, fig.height=12}
data_long = melt(data,id.vars = c("STORE","LONG","LAT","ZIP.CODE","CITY","TOTAL"))
# change the name of the newly created variables
names(data_long)[names(data_long) == 'variable'] <- "BRAND"
names(data_long)[names(data_long) == 'value'] <- "BRAND.SALE"
data_long = subset(data_long, select = -c(TOTAL))

sales_by_candy = data_long %>% 
  group_by(BRAND) %>% 
  summarise (sales_mean = mean(BRAND.SALE),
             sales_median = median(BRAND.SALE), 
             sales_sum = sum(BRAND.SALE), n = n()) 

sales_by_candy = as.data.frame(sales_by_candy)

# convert to long format
sales_by_candy_long= gather(sales_by_candy, sales_mean,sales_median,sales_sum,
                            sales_mean:sales_median:sales_sum, factor_key=TRUE)
names(sales_by_candy_long)[names(sales_by_candy_long) == 'sales_mean'] <- "Measure"
names(sales_by_candy_long)[names(sales_by_candy_long) == 'sales_median'] <- "value"

```


The top five successful brands and stores, respectively, are prtinted below: 
```{r brands_top, warning = FALSE}
sales_by_candy[order(-sales_by_candy$sales_mean),][1:5,]
candy_by_store[order(-candy_by_store$sales_mean),][1:5,]
```

we can now go ahead and visualize some basic statistics in this data. In the two plots below we can see that Pittsburgh is significantly lower in candy sale from the three other cities.the median candy sale for stores in washington is highest than other cities, and the mean is almost the same as Boston and Philadelphia, meaning that Washington does well constantly in all stores. 

```{r basic cities ,echo = FALSE,  fig.width=12, fig.height=5, warning=FALSE}

p1 = ggplot(aes(x = reorder(CITY,-value),  y= value), 
      data = candy_by_city_long[candy_by_city_long$Measure == "sales_sum",])+
geom_bar(fill = "blue",stat="identity",
          position = "dodge",width=.5,color=I('black'))+
  xlab("City")+ylab("Total Sale (USD)")

 

p2 = ggplot(aes(x = reorder(CITY,-value),  y= value), 
      data = candy_by_city_long[!candy_by_city_long$Measure == "sales_sum",])+
geom_bar(aes(fill = factor(Measure)),stat="identity",
          position = "dodge",width=.5,color=I('black'))+
  xlab("city")+ylab("Mean and Median Sales (USD)")

grid.arrange(p1,p2,ncol = 2)

```

We can now compare different Stores in their capacity for candy sale. In the plots below we can see that the number of Rite Aid stores is largesr, still, each Rite Aid Store perfoms bad on average. The best average sale belongs to BJs Wholesale Clubs and Target Stores. The number of Target Stores is 8th in rank in terms of frequency, meaning that Target stores are significant markets for candy sale in these four cities.  


```{r basic stores ,echo = FALSE,  fig.width=12, fig.height=12, warning=FALSE}

p3 = ggplot(aes(x = reorder(STORE,-n),  y= n), 
      data = candy_by_store_long[candy_by_store_long$Measure == "sales_sum",])+
geom_bar(fill = "blue",stat="identity",
          position = "dodge",width=.5,color=I('black'))+
  xlab("STORES")+ylab("Number of Stores")+
   coord_flip()

p4 = ggplot(aes(x = reorder(STORE,-value),  y= value), 
      data = candy_by_store_long[!candy_by_store_long$Measure == "sales_sum",])+
geom_bar(aes(fill = factor(Measure)),stat="identity",
          position = "dodge",width=.5,color=I('black'))+
  xlab("STORES")+ylab("Mean and Median Sales (USD)")+
   coord_flip()

grid.arrange(p3,p4,ncol = 2)
```

The brand sale plots below, indicate that Reese is the most successful overall. Kit-Kat and Kisses follow with a significant margin. 

```{r basic brands ,echo = FALSE,  fig.width=12, fig.height=12, warning=FALSE}

p5 = ggplot(aes(x = reorder(BRAND,-value),  y= value), 
      data = sales_by_candy_long[sales_by_candy_long$Measure == "sales_sum",])+
geom_bar(fill = "blue",stat="identity",
          position = "dodge",width=.5,color=I('black'))+
  xlab("Brands")+ylab("Total Brand Sale (USD)")+
   coord_flip()

p6 = ggplot(aes(x = reorder(BRAND,-value),  y= value), 
      data = sales_by_candy_long[!sales_by_candy_long$Measure == "sales_sum",])+
geom_bar(aes(fill = factor(Measure)),stat="identity",
          position = "dodge",width=.5,color=I('black'))+
  xlab("Brands")+ylab("Mean and Median Sales (USD)")+
   coord_flip()

grid.arrange(p5,p6,ncol = 2)
```


To see how each store performs in each city we should group by both store and city. Target store is Significantly high in all four cities although more successful in Boston. in philadelphia only BJs Wholesale club is nmore successful than Target stores.   

```{r city store, fig.width=7, fig.height=12, warning=FALSE}

candy_by_store_city = data %>% 
  group_by(STORE,CITY) %>% 
  summarise (sales_mean = mean(TOTAL),
             sales_median = median(TOTAL), 
             sales_sum = sum(TOTAL), n = n()) 

ggplot(aes(x = reorder(STORE,-sales_mean),  y= sales_mean), 
       data = candy_by_store_city)+
  geom_bar(aes(fill = factor(CITY)),stat="identity",
           position = "dodge",width=.5,color=I('black')) + 
  coord_flip()+ylab("Store")
```

# Which Brands sell more and in which cities?

Now let's convert the data to long format so that we can analyse each brand same as we did for each store. the plot below indicates how different brands compare in different cities. We can see that "Reese" is successful in all 4 cities.Some cities are specifically higher than others in some brands. For example, Hershey's Chocolate Assortment is significantly higher in Boston and DC. KitKat and Cadbury are highest in Boston while Philadelphia is more successful in Reese and Kisses. 

```{r brands2, fig.width=12, fig.height=12}
data_long = melt(data,id.vars = c("STORE","LONG","LAT","ZIP.CODE","CITY","TOTAL"))
# change the name of the newly created variables
names(data_long)[names(data_long) == 'variable'] <- "BRAND"
names(data_long)[names(data_long) == 'value'] <- "BRAND.SALE"
names(data_long)


p1<- ggplot(aes(x = BRAND,  y= BRAND.SALE), data = data_long) +
  stat_summary(aes(fill = factor(CITY)),colour = "black",fun.y=mean, geom="bar",
               position=position_dodge(0.6), width = 0.8, alpha = 0.7)+   
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+
  xlab("Candy Brand")+ylab("Mean Brand Sale(USD)")+
  labs(title = "Mean Sale for Different Brands and Cities")+
  scale_fill_discrete(guide = guide_legend(title = "CITY"))+ coord_flip()

p2<- ggplot(aes(x = BRAND,  y= BRAND.SALE), data = data_long) +
  stat_summary(aes(fill = factor(CITY)),colour = "black",fun.y=median, geom="bar",
               position=position_dodge(0.6), width = 0.8, alpha = 0.7)+   
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+
  xlab("Candy Brand")+ylab("Median Brand Sale (USD)")+
  labs(title = "Median Sale for Different Brands and Cities")+
  scale_fill_discrete(guide = guide_legend(title = "CITY"))+ coord_flip()

grid.arrange(p1,p2,ncol = 2)
```


# Which zip codes consume more candy on average?
We can now check the zip codes to see which ones sell more candies. I use the Census 2010 population data to normalize the candy sales on population. I found that three zipcodes are extremely higher than others so I removed them (i.e. "19112","19372","2199"). 
```{r zipcodes, fig.width=12, fig.height=12}

zipcode_pop = read.csv("C:/Users/sur216/Box Sync/school stuff/Udacity (sur216@psu.edu)/Data Analyst/p4_hershey/Zipcode-pop.csv",header = T)

colnames(zipcode_pop)[1]<- "ZIP.CODE"
candy_by_zipcode = data %>% 
  group_by(ZIP.CODE,CITY) %>% 
  summarise (sales_mean = mean(TOTAL),
             sales_median = median(TOTAL), 
             sales_sum = sum(TOTAL), n = n()) 


candy_by_zipcode=merge(candy_by_zipcode,zipcode_pop,by = "ZIP.CODE")
candy_by_zipcode = candy_by_zipcode[,-8]


ggplot(aes(x =reorder(as.character(ZIP.CODE),-sales_sum/X2010.Population),  
           y= sales_sum/X2010.Population), 
data =candy_by_zipcode[!candy_by_zipcode$ZIP.CODE%in%c("19112","19372","2199"),]) + 
geom_bar(aes(fill = factor(CITY)),colour = 'black', stat = "identity") +   
theme(axis.text.x=element_blank()) + 
facet_wrap(~CITY, scales = "free", ncol = 2)+
scale_fill_discrete(guide = guide_legend(title = "CITY"))+
  xlab("Zip Code")+ylab("Per Capita Candy Sale (USD)")+
  labs(title = "Per Capita Candy Sale in Different Zip Codes")
```

Now we can run a regression to examin the relationship between zipcode population and candy sale. It make sense to have higher candy consumption in zipcodes with higher populations. I will use the log-log transformation since the raw data will give us a fan-shaped scatter plot. 

```{r regression, fig.width=12, fig.height=6}

candy_fit = lm(sales_sum~X2010.Population, 
               data = candy_by_zipcode)
summary(candy_fit)

candy_zip_trans = transform(candy_by_zipcode,
                            log_pop = log(X2010.Population), 
                            log_sum = log(sales_sum))

ggplot(aes(x=log_pop, y =log_sum), data = candy_zip_trans)+ 
  geom_point(aes(colour = CITY, size = sales_median))+
  geom_smooth(method = 'lm', formula = y~x)+coord_cartesian(xlim = c(7, 11))
```

# Which brands sell together?

e first look into the simple correlations between different brands. we should use the wide format for correlations. As the correlation suggests, KitKat, Kisses, Reese, Cadbury, and Rolo are strongly correlated. The scatter plot is ordered through a hierarchial clustering and shows a number of interesting clusters.  

```{r correlation, fig.width=12, fig.height=12, , warning=FALSE}
# isolate all the candiy sales and ignore every other data
candies = data[,c(6:12,14:26,28:36)]
# create a correlation matrix
corr_candies = cor(candies)
# visualize the correlation matrix
corrplot(corr_candies, order = "hclust")
```


# How different markets compare together in terms of their candy sales?

To compare different markets in terms of their candy sale, I will first define a function to calculate the cosine similarity for us. The function below takes the data as matrix and returns a cosine similarity matrix. 

```{r define_cosine, fig.width=12, fig.height=12 , warning=FALSE}
# define the cosine similarity function
cosine <- function( x, y=NULL ) {

  if ( is.matrix(x) && is.null(y) ) {

    co = array(0,c(ncol(x),ncol(x)))
    f = colnames( x )
    dimnames(co) = list(f,f)

    for (i in 2:ncol(x)) {
      for (j in 1:(i-1)) {
        co[i,j] = cosine(x[,i], x[,j])
      }
    }
    co = co + t(co)
    diag(co) = 1

    return (as.matrix(co))

  } else if ( is.vector(x) && is.vector(y) ) {
    return ( crossprod(x,y) / sqrt( crossprod(x)*crossprod(y) ) )
  } else {
    stop("Error: input should be either a matrix or two vectors")
  }

}

```
Now we will apply this function to find the pairwise similarity matrix for markets. Using the levelplot() function we can visualiza this matrix in form of a heat map. we can now see interesting patterns in the heat map. for example, Target stores are distant from the BJs Wholesale Clubs and similar to Giant stores in terms of their candy sale. 

```{r similarity_markets, fig.width=12, fig.height=12, , warning=FALSE}

# concatenate the stores with their candy sale
store_candies = cbind(data$STORE,candies)
names(store_candies)[names(store_candies)=="data$STORE"] <- "STORE"

#aggregate the candy sales for each store type
stores <- store_candies %>%
group_by(STORE) %>%
  summarise_each(funs(mean))

stores = as.data.frame(stores)
rownames(stores)<- stores[,1]
stores_t = as.data.frame(t(stores)[-1,])
stores_t[is.na(stores_t)] <- 0
rownames(stores_t)<-c()


# create a matrix of the resulting dataframe
mat_stores = data.matrix(stores_t)
# create cosine similarity matrix
sim_stores = cosine(mat_stores,y=NULL)



# visualize the cosine similarity matrix
new.palette=colorRampPalette(c("black","red","yellow","white"),
                             space="rgb")

levelplot(sim_stores,col.regions=new.palette(20),
          scales=list(y=list(rot=0), x=list(rot=90)))
```

# How different neighborhoods compare in terms of candy consumption? 

Another interesting question to answer would be the geographical aspect of the markets. For each market we have the latitude and longitude data. We will use the ggmap() function to and assign colors to the stores to see which ones sell more. It looks like stores in the periphery are more successful in general. 

```{r maps_city, fig.width=13, fig.height=13, , warning=FALSE, message=FALSE}

# get  background maps for the four cities from google
phil <- get_map(location = "philadelphia",
                zoom = 11, source = "google", color = c("bw"))
bos  <- get_map(location = "boston",
                zoom = 12, source = "google", color = c("bw"))
dc  <- get_map(location = "washington dc",
               zoom = 12, source = "google", color = c("bw"))
pit  <- get_map(location = "pittsburgh",
                zoom = 11, source = "google", color = c("bw"))

p1<- ggmap(dc)+ geom_point(data = data, 
     aes(x=LONG, y=LAT, colour = log(TOTAL), size =TOTAL , alpha = 0.3))+ 
  scale_colour_gradient(limits=c(8, 12.72), low="red", high="green")+ 
  scale_size_continuous (name = c("Total Sale"))

p2<- ggmap(bos)+ geom_point(data = data, 
     aes(x=LONG, y=LAT, colour = log(TOTAL), size =TOTAL , alpha = 0.3))+ 
  scale_colour_gradient(limits=c(8, 12.73), low="red", high="green")+ 
  scale_size_continuous (name = c("Total Sale"))

p3<- ggmap(phil)+ geom_point(data = data, 
     aes(x=LONG, y=LAT, colour = log(TOTAL), size =TOTAL, alpha = 0.3))+ 
  scale_colour_gradient(limits=c(8, 12.73), low="red", high="green")+ 
  scale_size_continuous (name = c("Total Sale"))

p4<- ggmap(pit)+ geom_point(data = data, 
     aes(x=LONG, y=LAT, colour = log(TOTAL), size =TOTAL, alpha = 0.3))+ 
  scale_colour_gradient(limits=c(8, 12.73), low="red", high="green")+ 
  scale_size_continuous (name = c("Total Sale"))
  
grid.arrange(p1,p2,p3,p4,ncol = 2)
```

A more advanced way of going about the geographic aspect of the candy consumption is to find clusters of similar stores. That is, we first calculate a similarity matrix with the function that I previously define and then find clusters using spectral clustering. Every store in philadelphia will be compared to all other stores (pairwise) based on the candy sales for each store. 

Doing so for Philadelphia, we can clearly see that there is a clear spatial pattern with downtown and suburbia being almost the same and the neighborhoods between them following another candy consumption patter. This shows that geography is correlated with candy taste. 

```{r phily clusters, fig.width=12, fig.height=12, , warning=FALSE}

#spectral clustering for Phily

candies_phily = data.matrix(t(candies[data$CITY == "Philadelphia",]))
sim_phily = cosine(candies_phily, y=NULL)
sc_phily<- spectralClustering(sim_phily, 3, type = 3)
phily_markets_SC = cbind(sc_phily,
                         data[data$CITY == "Philadelphia",c("LONG","LAT")])

ggmap(phil) + 
  geom_point(data = phily_markets_SC, 
             aes(x=LONG, y=LAT, colour = factor(sc_phily),alpha= 0.5, size = 2))

```

although the plot above is informative, if we intend to see the neighborhoods' behavior in terms of candy consumption, it would be a better idea to divide the city to a number of spatial bins, i.e. geographic areas with similar dimensions. After doing this, we can average teh candy consumption in each area and run the spectral clustering algorithm once again for these spacial bins. 

```{r spatial bins phily , warning=FALSE,fig.width=16, fig.height=16}

phily_data = cbind(candies[data$CITY == "Philadelphia",],
                   data[data$CITY == "Philadelphia",c("LONG","LAT")])

# divide the philly area to a 90*90 grid of squares 
phily_data$lat_bins = cut(phily_data$LAT,breaks = 90)
phily_data$lon_bins = cut(phily_data$LONG,breaks = 90)

# The candy sale in each square will
# be averaged nd assigned to that square
phily_bins<- phily_data%>%
  group_by(lat_bins,lon_bins) %>%
  summarise_each(funs(mean))


#separate the candy rows and standardize
phily_bins_cands=t(apply(as.data.frame(phily_bins)[3:31], 1, 
                        function(x)(x-min(x))/(max(x)-min(x))))

#create similarity matrix
phily_bins_cands_t = data.matrix(t(phily_bins_cands))
sim_phily_bins = cosine(phily_bins_cands_t, y=NULL)
sc_phily_bins<- spectralClustering(sim_phily_bins, 3, type = 3)


phily_clusters = data.frame(phily_bins,sc_phily_bins)

# extract the minimum and maximum lat and long for 
#each square (so that we can draw them as rectangles later on)

maxlon = sapply(str_extract_all
                (as.character(phily_clusters[,2]),
                "\\d+\\.*\\d*"), "[[", 1)
minlon =  sapply(str_extract_all
                 (as.character(phily_clusters[,2]),
                 "\\d+\\.*\\d*"), "[[", 2)
maxlat = as.numeric(sapply(str_extract_all
                           (as.character(phily_clusters[,1])
                           , "\\d+\\.*\\d*"), "[[", 1))
minlat =  as.numeric(sapply(str_extract_all
                            (as.character(phily_clusters[,1]),
                            "\\d+\\.*\\d*"), "[[", 2))
minlon=  as.numeric(minlon)*-1
maxlon = as.numeric(maxlon)*-1

phily_clusters = data.frame(phily_clusters,minlat,maxlat,minlon,maxlon)

# draw the resulting rectangles from the previous step
ggmap(phil) + 
  geom_rect(data=phily_clusters, 
            mapping=aes(xmin=minlon, xmax=maxlon, ymin=minlat, ymax=maxlat,             
            fill=factor(sc_phily_bins)), alpha=0.4,inherit.aes=FALSE)+
theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

# Final plots and summary

In this section I will select the three most informative plots that I hjave made through out the course of exploratory data analysis on the candy data. First, the bar plot for each market plotted below indicates that the target is highest on average in both median values and mean values for all 4 citiesExcept for Philadelphia, as explained earlier.  

```{r citystore_clean,echo = FALSE, fig.width=12, fig.height=12}

candy_by_store = data %>% 
  group_by(STORE,CITY) %>% 
  summarise (sales_mean = mean(TOTAL),
             sales_median = median(TOTAL), 
             sales_sum = sum(TOTAL), n = n()) 

p1<- ggplot(aes(x = reorder(STORE,-sales_mean),  
                y= sales_mean), data = candy_by_store)+
  geom_bar(aes(fill = factor(CITY)),stat="identity",
           position = "dodge",width=.5, alpha = 0.8,color=I('black')) + 
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+
  xlab("STORES")+
  ylab("Mean Candy Sale (USD)")+
  labs(title = "Mean Candy Sale for Different Stores and Cities (USD)")+
  scale_fill_discrete(guide = guide_legend(title = "CITY")) + coord_flip()

p2<- ggplot(aes(x = reorder(STORE,-sales_median),  
                y= sales_median), data = candy_by_store)+
  geom_bar(aes(fill = factor(CITY)),stat="identity",
           position = "dodge",width=.5, alpha = 0.8,color=I('black')) + 
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+
  xlab("STORES")+
  ylab("Median Candy Sale (USD)")+
  labs(title = "Median Candy Sale for Different Stores and Cities")+
  scale_fill_discrete(guide = guide_legend(title = "CITY")) + coord_flip()

grid.arrange(p1,p2,ncol = 2)
```


For the regression, we limit the x axis to see the bulk of the data more clearly. we assign node size to the points which corresponds with the median sales in stores. We can now clearly see that there is a positive linear association between the log population and log candy sales in zipcodes.

```{r regression_clean,echo = FALSE, fig.width=12, fig.height=6}

candy_fit = lm(sales_sum~X2010.Population, data = candy_by_zipcode)


ggplot(aes(x=X2010.Population, y =sales_sum), data = candy_by_zipcode)+ 
  geom_point(aes(colour = CITY, size = sales_median), alpha = 0.7)+ 
  scale_color_brewer(palette="Set1")+
  geom_smooth(method = 'lm', formula = y~x)+ scale_x_log10()+scale_y_log10()+
  coord_cartesian(xlim = c(1e+03, 5e+04), ylim = c(1e+04, 2e+06))+
  scale_size_continuous (name = "MEDIAN")+
  xlab("Population")+ylab("Total Candy Sale (USD)")+
  labs(title = "The Relationship between population and candy sale in different zip codes")
```


At last, we apply the spectral clustering method to all four cities to investigate patterns of chocolate use in the city. We can see that washington can be divided into two part, the east and west. Recall that washington is also racially divided in this manner. Philadelphia also has this clear spatial clusters. For Pittsburgh, it seems that the neighborhoods are not as distinct, however some clear green points can be seen closer to the center. Boston as well shows some similarity in candy consumption in south and west. 

```{r maps_clusters,echo = FALSE, fig.width=16, fig.height=16, , warning=FALSE}

#spectral clustering for Pittsburgh
candies_pitt = data.matrix(t(candies[data$CITY == "Pittsburgh",]))
sim_pitt = cosine(candies_pitt, y=NULL)
sc_pitt<- spectralClustering(sim_pitt, 3, type = 3)
pitt_markets_SC = cbind(sc_pitt,data[data$CITY == "Pittsburgh",c("LONG","LAT")])

#spectral clustering for DC
candies_dc = data.matrix(t(candies[data$CITY == "Washington",]))
sim_dc = cosine(candies_dc, y=NULL)
sc_dc<- spectralClustering(sim_dc, 3, type = 3)
dc_markets_SC = cbind(sc_dc,data[data$CITY == "Washington",c("LONG","LAT")])

#spectral clustering for Boston
candies_bos = data.matrix(t(candies[data$CITY == "Boston",]))
sim_bos = cosine(candies_bos, y=NULL)
sc_bos<- spectralClustering(sim_bos, 3, type = 3)
bos_markets_SC = cbind(sc_bos,data[data$CITY == "Boston",c("LONG","LAT")])

p1<- ggmap(phil) + 
  geom_point(data = phily_markets_SC, 
             aes(x=LONG, y=LAT, colour = factor(sc_phily),size = 0.7, alpha = 0.5))+
   guides(alpha=FALSE,size = FALSE)+scale_colour_discrete(name = "Clusters of Stores")+
   labs(title = "Clusters derivied from the similarity of stores in Philadelphia", size=2.5)
  

p2<- ggmap(pit) + 
  geom_point(data = pitt_markets_SC, 
             aes(x=LONG, y=LAT, colour = factor(sc_pitt),size = 0.7, alpha = 0.5))+
   guides(alpha=FALSE,size = FALSE)+scale_colour_discrete(name = "Clusters of Stores")+
   labs(title = "Clusters derivied from the similarity of stores in Pittsburgh", size=2.5)

p3<- ggmap(dc) + 
  geom_point(data = dc_markets_SC, 
             aes(x=LONG, y=LAT, colour = factor(sc_dc),size = 0.7, alpha = 0.5))+
   guides(alpha=FALSE,size = FALSE)+scale_colour_discrete(name = "Clusters of Stores")+
   labs(title = "Clusters derivied from the similarity of stores in Washington DC", size=2.5)

p4<- ggmap(bos) + 
  geom_point(data = bos_markets_SC, 
             aes(x=LONG, y=LAT, colour = factor(sc_bos),size = 0.7, alpha = 0.5))+
   guides(alpha=FALSE,size = FALSE)+
   scale_colour_discrete(name = "Clusters of Stores")+
   labs(title = "Clusters derivied from the similarity of stores in Boston", size=2.5)

grid.arrange(p1,p2,p3,p4,ncol = 2)

```

# Reflection

Our data analysis revewled some interesting patterns in the candy consumption data set. We found out which markets are more likely to sell more candy, We calos learnt how different brands compare in different cities. More importantly, we found out about cities and neighborhoods in each city. We found that there are clear candy consumption patterns in the four cities that we focused on. 

At the same time, thhere were a number of limitations that cannot be neglected. First, the number of data points for DC and Boston were significantly lower than the two other. This made it harder for a comprehensive comparison between the four cities. Also, There were a number of candies that were not included in the dataset that coule inform more about the tastes in different cities and neighborhoods and improve our clusterings. Future work can take the zip code demographic data into consideration and investigate the associations between different factors such as income, racial composition and etc. 

Although the data was not large (i.e. 1253 entities and 37 variables) There were a numbe rof challenges associated with working with this dataset. First and foremost, the multitude of categorical variables (i.e. city, zip codes,stores) reuired one to create a large number of visualizations to understand the dynamics of the dataset. The second challenge was the geographic component of the data. Finding geographical patterns within the dataset, although was ultimately achieved, required a wide range of techniques including calculating cosine similarities, clustering, spatial bins, and a number of visualization techniques. My personal take from this data, was the interesting association that I found between the neighborhoods and the taste of their residents. 